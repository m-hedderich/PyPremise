{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyPremise Example: Word Embedding Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyPremise enables easy identification of interpretable patterns in classifier performance. Beyond basic token matching, we can use word embeddings to capture semantic similarity between words — for example, allowing the model to group \"photo\", \"picture\", and \"image\" together.\n",
    "\n",
    "In this notebook, we explore how to incorporate FastText or other custom word embeddings into PyPremise for richer pattern discovery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Use Word Embeddings?**\n",
    "\n",
    "Without embeddings, pattern discovery only works with exact word matches. With embeddings, semantically similar words can be grouped together:\n",
    "```\n",
    "(\"photo\" or \"picture\" or \"image\") → ✔ With embeddings\n",
    "(\"photo\" only)                   → ✖ Without embeddings\n",
    "```\n",
    "Word embeddings like FastText can represent each word as a vector in a high-dimensional space, where similar words are close together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use FastText .bin embeddings with PyPremise, make sure the fasttext Python package is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mounts/work/xinyuan/pypremise-dev-internal/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing all the packages we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypremise import Premise, data_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’ll also need to have a vocabulary mapping prepared from your tokenized data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should be created when you construct your PremiseInstance objects\n",
    "# voc_index_to_token: mapping from token index to token string\n",
    "premise_instances,  _, voc_index_to_token = data_loaders.get_dummy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyPremise provides built-in support for FastText .bin files (e.g. from fasttext.cc). Use this if your vocabulary is in English and you want good out-of-the-box semantic clustering.\n",
    "\n",
    "Downloading FastText Word Embeddings\n",
    "To use pretrained FastText word embeddings, you first need to download the .bin file.\n",
    "\n",
    " Download link (English, 300 dimensions):\n",
    "https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "\n",
    "Website overview:\n",
    "FastText official vectors page:\n",
    "https://fasttext.cc/docs/en/crawl-vectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FastText model, this might take a bit.\n",
      "FastText loaded. Mapping the tokens to their embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Point this to your downloaded FastText binary file\n",
    "fasttext_path = \"/path/to/cc.en.300.bin\"  # e.g. 300-dimensional English embeddings\n",
    "\n",
    "embedding_index_to_vector, embedding_dimensionality = data_loaders.create_fasttext_mapping(\n",
    "    fasttext_path, voc_index_to_token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then initialize Premise with these embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = Premise(\n",
    "    voc_index_to_token=voc_index_to_token,\n",
    "    embedding_index_to_vector=embedding_index_to_vector,\n",
    "    embedding_dimensionality=embedding_dimensionality,\n",
    "    max_neighbor_distance=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells Premise to use semantic proximity (via the embedding space) when searching for patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that Premise is embedding-aware, you can extract patterns as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(How) and (many) towards group 0 (Instances: 9 in group 0, 0 in group 1)\n",
      "(was) and (taken) and (When) and (photo-or-photograph) towards group 1 (Instances: 0 in group 0, 7 in group 1)\n"
     ]
    }
   ],
   "source": [
    "patterns = premise.find_patterns(premise_instances)\n",
    "for p in patterns:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use any other word embeddings of your choice. You just need to provide to Premise the following:\n",
    "| **Parameter**                 | **Description**                                                                                              |\n",
    "|------------------------------|--------------------------------------------------------------------------------------------------------------|\n",
    "| `embedding_dimensionality`   | The dimensionality of the embedding vectors. Must match the number of dimensions (e.g., 300 for `cc.en.300.bin`). |\n",
    "| `max_neighbor_distance`      | How many neighbors to look at. Should be a number > 0.                                                       |\n",
    "| `embedding_index_to_vector`  | A mapping from an index to its corresponding embedding. Use `voc_token_to_index` to look up token indices.   |\n",
    "\n",
    "**Note:** Make sure most tokens in your `voc_index_to_token` have a corresponding embedding to ensure good coverage.\n",
    "                                            |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pypremise",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
